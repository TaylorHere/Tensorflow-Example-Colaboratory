{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow-Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rYgco5ZS-WnI8kfcNS0LRdBdqviQsEi4",
      "authorship_tag": "ABX9TyOodaAlKHp9xG0Ww67AaCyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaylorHere/Tensorflow-Example-Colaboratory/blob/master/Tensorflow_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6fKbKlfFJyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr9xekd6FlDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "964846a0-94cf-413f-c320-78a776281c78"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAYzphMPF4TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18ef02d6-4c87-4d7f-dadc-cfe9f9f021cb"
      },
      "source": [
        "hello = tf.constant('Hello, TensorFlow!')\n",
        "sess = tf.Session()\n",
        "print(sess.run(hello))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello, TensorFlow!'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJyZ6OmzGd88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "acb89c75-2cd7-4aef-a04f-46d402d95e38"
      },
      "source": [
        "'''\n",
        "Basic Operations example using TensorFlow library.\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Basic constant operations\n",
        "# The value returned by the constructor represents the output\n",
        "# of the Constant op.\n",
        "a = tf.constant(2)\n",
        "b = tf.constant(3)\n",
        "\n",
        "# Launch the default graph.\n",
        "with tf.Session() as sess:\n",
        "    print(\"a=2, b=3\")\n",
        "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
        "    print(\"Multiplication with constants: %i\" % sess.run(a*b))\n",
        "\n",
        "# Basic Operations with variable as graph input\n",
        "# The value returned by the constructor represents the output\n",
        "# of the Variable op. (define as input when running session)\n",
        "# tf Graph input\n",
        "a = tf.placeholder(tf.int16)\n",
        "b = tf.placeholder(tf.int16)\n",
        "\n",
        "# Define some operations\n",
        "add = tf.add(a, b)\n",
        "mul = tf.multiply(a, b)\n",
        "\n",
        "# Launch the default graph.\n",
        "with tf.Session() as sess:\n",
        "    # Run every operation with variable input\n",
        "    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n",
        "    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# More in details:\n",
        "# Matrix Multiplication from TensorFlow official tutorial\n",
        "\n",
        "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
        "# added as a node to the default graph.\n",
        "#\n",
        "# The value returned by the constructor represents the output\n",
        "# of the Constant op.\n",
        "matrix1 = tf.constant([[3., 3.]])\n",
        "\n",
        "# Create another Constant that produces a 2x1 matrix.\n",
        "matrix2 = tf.constant([[2.],[2.]])\n",
        "\n",
        "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
        "# The returned value, 'product', represents the result of the matrix\n",
        "# multiplication.\n",
        "product = tf.matmul(matrix1, matrix2)\n",
        "\n",
        "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
        "# which represents the output of the matmul op.  This indicates to the call\n",
        "# that we want to get the output of the matmul op back.\n",
        "#\n",
        "# All inputs needed by the op are run automatically by the session.  They\n",
        "# typically are run in parallel.\n",
        "#\n",
        "# The call 'run(product)' thus causes the execution of threes ops in the\n",
        "# graph: the two constants and matmul.\n",
        "#\n",
        "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
        "with tf.Session() as sess:\n",
        "    result = sess.run(product)\n",
        "    print(result)\n",
        "    # ==> [[ 12.]]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=2, b=3\n",
            "Addition with constants: 5\n",
            "Multiplication with constants: 6\n",
            "Addition with variables: 5\n",
            "Multiplication with variables: 6\n",
            "[[12.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN-bBYSkIlh9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "6e59eefb-d8a3-42cd-e1f5-ba6c3a4e5880"
      },
      "source": [
        "'''\n",
        "A linear regression learning algorithm example using TensorFlow library.\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "rng = numpy.random\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 1000\n",
        "display_step = 50\n",
        "\n",
        "# Training Data\n",
        "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
        "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
        "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
        "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
        "n_samples = train_X.shape[0]\n",
        "\n",
        "# tf Graph Input\n",
        "X = tf.placeholder(\"float\")\n",
        "Y = tf.placeholder(\"float\")\n",
        "\n",
        "# Set model weights\n",
        "W = tf.Variable(rng.randn(), name=\"weight\")\n",
        "b = tf.Variable(rng.randn(), name=\"bias\")\n",
        "\n",
        "# Construct a linear model\n",
        "pred = tf.add(tf.multiply(X, W), b)\n",
        "\n",
        "# Mean squared error\n",
        "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
        "# Gradient descent\n",
        "#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    # Fit all training data\n",
        "    for epoch in range(training_epochs):\n",
        "        for (x, y) in zip(train_X, train_Y):\n",
        "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
        "\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
        "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
        "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
        "\n",
        "    # Graphic display\n",
        "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
        "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Testing example, as requested (Issue #2)\n",
        "    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n",
        "    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n",
        "\n",
        "    print(\"Testing... (Mean square loss Comparison)\")\n",
        "    testing_cost = sess.run(\n",
        "        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),\n",
        "        feed_dict={X: test_X, Y: test_Y})  # same function as cost above\n",
        "    print(\"Testing cost=\", testing_cost)\n",
        "    print(\"Absolute mean square loss difference:\", abs(\n",
        "        training_cost - testing_cost))\n",
        "\n",
        "    plt.plot(test_X, test_Y, 'bo', label='Testing data')\n",
        "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0050 cost= 0.077892393 W= 0.2330559 b= 0.9204378\n",
            "Epoch: 0100 cost= 0.077789724 W= 0.23403928 b= 0.91336304\n",
            "Epoch: 0150 cost= 0.077698827 W= 0.23496428 b= 0.90670884\n",
            "Epoch: 0200 cost= 0.077618316 W= 0.23583424 b= 0.90045005\n",
            "Epoch: 0250 cost= 0.077547066 W= 0.2366525 b= 0.8945637\n",
            "Epoch: 0300 cost= 0.077483930 W= 0.23742218 b= 0.8890267\n",
            "Epoch: 0350 cost= 0.077428043 W= 0.238146 b= 0.8838194\n",
            "Epoch: 0400 cost= 0.077378526 W= 0.23882677 b= 0.87892216\n",
            "Epoch: 0450 cost= 0.077334687 W= 0.23946707 b= 0.8743161\n",
            "Epoch: 0500 cost= 0.077295840 W= 0.24006937 b= 0.86998314\n",
            "Epoch: 0550 cost= 0.077261411 W= 0.24063574 b= 0.8659087\n",
            "Epoch: 0600 cost= 0.077230938 W= 0.24116847 b= 0.862076\n",
            "Epoch: 0650 cost= 0.077203929 W= 0.24166949 b= 0.8584715\n",
            "Epoch: 0700 cost= 0.077179991 W= 0.24214083 b= 0.8550813\n",
            "Epoch: 0750 cost= 0.077158771 W= 0.24258406 b= 0.8518924\n",
            "Epoch: 0800 cost= 0.077139966 W= 0.24300098 b= 0.84889305\n",
            "Epoch: 0850 cost= 0.077123292 W= 0.2433931 b= 0.84607226\n",
            "Epoch: 0900 cost= 0.077108510 W= 0.243762 b= 0.8434186\n",
            "Epoch: 0950 cost= 0.077095419 W= 0.24410884 b= 0.84092325\n",
            "Epoch: 1000 cost= 0.077083789 W= 0.24443512 b= 0.83857584\n",
            "Optimization Finished!\n",
            "Training cost= 0.07708379 W= 0.24443512 b= 0.83857584 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFLMNCJiFyCgU5E5BIMdCIil/j/0teKmEBMD3kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing... (Mean square loss Comparison)\n",
            "Testing cost= 0.08023864\n",
            "Absolute mean square loss difference: 0.0031548515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/D/S3iEGNaZCZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_G9bzPnI4PE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "d8c38d45-4df6-4bbf-9308-edd340bff481"
      },
      "source": [
        "'''\n",
        "A logistic regression learning algorithm example using TensorFlow library.\n",
        "This example is using the MNIST database of handwritten digits\n",
        "(http://yann.lecun.com/exdb/mnist/)\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 25\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# tf Graph Input\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
        "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# Set model weights\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# Minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
        "                                                          y: batch_ys})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\"\"\" K-Means.\n",
        "Implement K-Means algorithm with TensorFlow, and apply it to classify\n",
        "handwritten digit images. This example is using the MNIST database of\n",
        "handwritten digits as training samples (http://yann.lecun.com/exdb/mnist/).\n",
        "Note: This example requires TensorFlow v1.1.0 or over.\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.factorization import KMeans\n",
        "\n",
        "# Ignore all GPUs, tf k-means does not benefit from it.\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "full_data_x = mnist.train.images\n",
        "\n",
        "# Parameters\n",
        "num_steps = 50 # Total steps to train\n",
        "batch_size = 1024 # The number of samples per batch\n",
        "k = 25 # The number of clusters\n",
        "num_classes = 10 # The 10 digits\n",
        "num_features = 784 # Each image is 28x28 pixels\n",
        "\n",
        "# Input images\n",
        "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
        "# Labels (for assigning a label to a centroid and testing)\n",
        "Y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "# K-Means Parameters\n",
        "kmeans = KMeans(inputs=X, num_clusters=k, distance_metric='cosine',\n",
        "                use_mini_batch=True)\n",
        "\n",
        "# Build KMeans graph\n",
        "training_graph = kmeans.training_graph()\n",
        "\n",
        "if len(training_graph) > 6: # Tensorflow 1.4+\n",
        "    (all_scores, cluster_idx, scores, cluster_centers_initialized,\n",
        "     cluster_centers_var, init_op, train_op) = training_graph\n",
        "else:\n",
        "    (all_scores, cluster_idx, scores, cluster_centers_initialized,\n",
        "     init_op, train_op) = training_graph\n",
        "\n",
        "cluster_idx = cluster_idx[0] # fix for cluster_idx being a tuple\n",
        "avg_distance = tf.reduce_mean(scores)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init_vars = tf.global_variables_initializer()\n",
        "\n",
        "# Start TensorFlow session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Run the initializer\n",
        "sess.run(init_vars, feed_dict={X: full_data_x})\n",
        "sess.run(init_op, feed_dict={X: full_data_x})\n",
        "\n",
        "# Training\n",
        "for i in range(1, num_steps + 1):\n",
        "    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],\n",
        "                         feed_dict={X: full_data_x})\n",
        "    if i % 10 == 0 or i == 1:\n",
        "        print(\"Step %i, Avg Distance: %f\" % (i, d))\n",
        "\n",
        "# Assign a label to each centroid\n",
        "# Count total number of labels per centroid, using the label of each training\n",
        "# sample to their closest centroid (given by 'idx')\n",
        "counts = np.zeros(shape=(k, num_classes))\n",
        "for i in range(len(idx)):\n",
        "    counts[idx[i]] += mnist.train.labels[i]\n",
        "# Assign the most frequent label to the centroid\n",
        "labels_map = [np.argmax(c) for c in counts]\n",
        "labels_map = tf.convert_to_tensor(labels_map)\n",
        "\n",
        "# Evaluation ops\n",
        "# Lookup: centroid_id -> label\n",
        "cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)\n",
        "# Compute accuracy\n",
        "correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, 1), tf.int32))\n",
        "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Test Model\n",
        "test_x, test_y = mnist.test.images, mnist.test.labels\n",
        "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost= 1.183714604\n",
            "Epoch: 0002 cost= 0.665267493\n",
            "Epoch: 0003 cost= 0.552805119\n",
            "Epoch: 0004 cost= 0.498684658\n",
            "Epoch: 0005 cost= 0.465487297\n",
            "Epoch: 0006 cost= 0.442625228\n",
            "Epoch: 0007 cost= 0.425550361\n",
            "Epoch: 0008 cost= 0.412123397\n",
            "Epoch: 0009 cost= 0.401359793\n",
            "Epoch: 0010 cost= 0.392404863\n",
            "Epoch: 0011 cost= 0.384785336\n",
            "Epoch: 0012 cost= 0.378183064\n",
            "Epoch: 0013 cost= 0.372401498\n",
            "Epoch: 0014 cost= 0.367278265\n",
            "Epoch: 0015 cost= 0.362749341\n",
            "Epoch: 0016 cost= 0.358618448\n",
            "Epoch: 0017 cost= 0.354850122\n",
            "Epoch: 0018 cost= 0.351473243\n",
            "Epoch: 0019 cost= 0.348360174\n",
            "Epoch: 0020 cost= 0.345448354\n",
            "Epoch: 0021 cost= 0.342742469\n",
            "Epoch: 0022 cost= 0.340242068\n",
            "Epoch: 0023 cost= 0.337928403\n",
            "Epoch: 0024 cost= 0.335759588\n",
            "Epoch: 0025 cost= 0.333720295\n",
            "Optimization Finished!\n",
            "Accuracy: 0.9144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVcg5LAiJNCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d5eab25-e62d-42b8-ef20-449a337c7b68"
      },
      "source": [
        "'''\n",
        "A nearest neighbor learning algorithm example using TensorFlow library.\n",
        "This example is using the MNIST database of handwritten digits\n",
        "(http://yann.lecun.com/exdb/mnist/)\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# In this example, we limit mnist data\n",
        "Xtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates)\n",
        "Xte, Yte = mnist.test.next_batch(200) #200 for testing\n",
        "\n",
        "# tf Graph Input\n",
        "xtr = tf.placeholder(\"float\", [None, 784])\n",
        "xte = tf.placeholder(\"float\", [784])\n",
        "\n",
        "# Nearest Neighbor calculation using L1 Distance\n",
        "# Calculate L1 Distance\n",
        "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n",
        "# Prediction: Get min distance index (Nearest neighbor)\n",
        "pred = tf.arg_min(distance, 0)\n",
        "\n",
        "accuracy = 0.\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    # loop over test data\n",
        "    for i in range(len(Xte)):\n",
        "        # Get nearest neighbor\n",
        "        nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n",
        "        # Get nearest neighbor class label and compare it to its true label\n",
        "        print(\"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]), \\\n",
        "            \"True Class:\", np.argmax(Yte[i]))\n",
        "        # Calculate accuracy\n",
        "        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n",
        "            accuracy += 1./len(Xte)\n",
        "    print(\"Done!\")\n",
        "    print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Test 0 Prediction: 1 True Class: 1\n",
            "Test 1 Prediction: 4 True Class: 9\n",
            "Test 2 Prediction: 8 True Class: 8\n",
            "Test 3 Prediction: 4 True Class: 4\n",
            "Test 4 Prediction: 8 True Class: 8\n",
            "Test 5 Prediction: 1 True Class: 1\n",
            "Test 6 Prediction: 2 True Class: 2\n",
            "Test 7 Prediction: 4 True Class: 4\n",
            "Test 8 Prediction: 9 True Class: 9\n",
            "Test 9 Prediction: 9 True Class: 9\n",
            "Test 10 Prediction: 1 True Class: 1\n",
            "Test 11 Prediction: 0 True Class: 0\n",
            "Test 12 Prediction: 3 True Class: 3\n",
            "Test 13 Prediction: 4 True Class: 4\n",
            "Test 14 Prediction: 9 True Class: 9\n",
            "Test 15 Prediction: 3 True Class: 3\n",
            "Test 16 Prediction: 1 True Class: 1\n",
            "Test 17 Prediction: 1 True Class: 7\n",
            "Test 18 Prediction: 3 True Class: 8\n",
            "Test 19 Prediction: 3 True Class: 3\n",
            "Test 20 Prediction: 2 True Class: 2\n",
            "Test 21 Prediction: 5 True Class: 5\n",
            "Test 22 Prediction: 7 True Class: 7\n",
            "Test 23 Prediction: 8 True Class: 8\n",
            "Test 24 Prediction: 3 True Class: 3\n",
            "Test 25 Prediction: 7 True Class: 7\n",
            "Test 26 Prediction: 6 True Class: 6\n",
            "Test 27 Prediction: 2 True Class: 2\n",
            "Test 28 Prediction: 2 True Class: 2\n",
            "Test 29 Prediction: 3 True Class: 3\n",
            "Test 30 Prediction: 0 True Class: 0\n",
            "Test 31 Prediction: 5 True Class: 5\n",
            "Test 32 Prediction: 1 True Class: 1\n",
            "Test 33 Prediction: 1 True Class: 1\n",
            "Test 34 Prediction: 0 True Class: 0\n",
            "Test 35 Prediction: 9 True Class: 9\n",
            "Test 36 Prediction: 9 True Class: 9\n",
            "Test 37 Prediction: 6 True Class: 6\n",
            "Test 38 Prediction: 5 True Class: 5\n",
            "Test 39 Prediction: 1 True Class: 1\n",
            "Test 40 Prediction: 2 True Class: 5\n",
            "Test 41 Prediction: 5 True Class: 5\n",
            "Test 42 Prediction: 5 True Class: 5\n",
            "Test 43 Prediction: 7 True Class: 7\n",
            "Test 44 Prediction: 6 True Class: 6\n",
            "Test 45 Prediction: 0 True Class: 0\n",
            "Test 46 Prediction: 2 True Class: 2\n",
            "Test 47 Prediction: 6 True Class: 6\n",
            "Test 48 Prediction: 3 True Class: 3\n",
            "Test 49 Prediction: 7 True Class: 2\n",
            "Test 50 Prediction: 4 True Class: 4\n",
            "Test 51 Prediction: 9 True Class: 4\n",
            "Test 52 Prediction: 4 True Class: 4\n",
            "Test 53 Prediction: 0 True Class: 0\n",
            "Test 54 Prediction: 0 True Class: 0\n",
            "Test 55 Prediction: 9 True Class: 9\n",
            "Test 56 Prediction: 2 True Class: 2\n",
            "Test 57 Prediction: 2 True Class: 2\n",
            "Test 58 Prediction: 9 True Class: 9\n",
            "Test 59 Prediction: 2 True Class: 2\n",
            "Test 60 Prediction: 9 True Class: 9\n",
            "Test 61 Prediction: 6 True Class: 6\n",
            "Test 62 Prediction: 9 True Class: 4\n",
            "Test 63 Prediction: 1 True Class: 1\n",
            "Test 64 Prediction: 8 True Class: 8\n",
            "Test 65 Prediction: 3 True Class: 3\n",
            "Test 66 Prediction: 3 True Class: 3\n",
            "Test 67 Prediction: 4 True Class: 4\n",
            "Test 68 Prediction: 6 True Class: 6\n",
            "Test 69 Prediction: 3 True Class: 3\n",
            "Test 70 Prediction: 2 True Class: 2\n",
            "Test 71 Prediction: 5 True Class: 8\n",
            "Test 72 Prediction: 5 True Class: 5\n",
            "Test 73 Prediction: 9 True Class: 9\n",
            "Test 74 Prediction: 8 True Class: 8\n",
            "Test 75 Prediction: 8 True Class: 8\n",
            "Test 76 Prediction: 8 True Class: 8\n",
            "Test 77 Prediction: 3 True Class: 3\n",
            "Test 78 Prediction: 6 True Class: 6\n",
            "Test 79 Prediction: 0 True Class: 0\n",
            "Test 80 Prediction: 4 True Class: 4\n",
            "Test 81 Prediction: 9 True Class: 9\n",
            "Test 82 Prediction: 8 True Class: 8\n",
            "Test 83 Prediction: 2 True Class: 2\n",
            "Test 84 Prediction: 7 True Class: 7\n",
            "Test 85 Prediction: 3 True Class: 3\n",
            "Test 86 Prediction: 2 True Class: 2\n",
            "Test 87 Prediction: 6 True Class: 8\n",
            "Test 88 Prediction: 4 True Class: 4\n",
            "Test 89 Prediction: 4 True Class: 4\n",
            "Test 90 Prediction: 8 True Class: 8\n",
            "Test 91 Prediction: 0 True Class: 0\n",
            "Test 92 Prediction: 2 True Class: 2\n",
            "Test 93 Prediction: 4 True Class: 4\n",
            "Test 94 Prediction: 1 True Class: 1\n",
            "Test 95 Prediction: 1 True Class: 1\n",
            "Test 96 Prediction: 4 True Class: 4\n",
            "Test 97 Prediction: 1 True Class: 4\n",
            "Test 98 Prediction: 7 True Class: 7\n",
            "Test 99 Prediction: 9 True Class: 9\n",
            "Test 100 Prediction: 9 True Class: 4\n",
            "Test 101 Prediction: 2 True Class: 2\n",
            "Test 102 Prediction: 6 True Class: 6\n",
            "Test 103 Prediction: 1 True Class: 1\n",
            "Test 104 Prediction: 7 True Class: 7\n",
            "Test 105 Prediction: 4 True Class: 4\n",
            "Test 106 Prediction: 4 True Class: 4\n",
            "Test 107 Prediction: 6 True Class: 6\n",
            "Test 108 Prediction: 7 True Class: 3\n",
            "Test 109 Prediction: 5 True Class: 5\n",
            "Test 110 Prediction: 2 True Class: 2\n",
            "Test 111 Prediction: 4 True Class: 4\n",
            "Test 112 Prediction: 2 True Class: 2\n",
            "Test 113 Prediction: 7 True Class: 7\n",
            "Test 114 Prediction: 5 True Class: 5\n",
            "Test 115 Prediction: 3 True Class: 3\n",
            "Test 116 Prediction: 9 True Class: 9\n",
            "Test 117 Prediction: 1 True Class: 1\n",
            "Test 118 Prediction: 1 True Class: 1\n",
            "Test 119 Prediction: 4 True Class: 4\n",
            "Test 120 Prediction: 9 True Class: 9\n",
            "Test 121 Prediction: 0 True Class: 0\n",
            "Test 122 Prediction: 4 True Class: 4\n",
            "Test 123 Prediction: 3 True Class: 3\n",
            "Test 124 Prediction: 7 True Class: 7\n",
            "Test 125 Prediction: 0 True Class: 0\n",
            "Test 126 Prediction: 9 True Class: 9\n",
            "Test 127 Prediction: 6 True Class: 4\n",
            "Test 128 Prediction: 4 True Class: 4\n",
            "Test 129 Prediction: 1 True Class: 1\n",
            "Test 130 Prediction: 6 True Class: 6\n",
            "Test 131 Prediction: 4 True Class: 4\n",
            "Test 132 Prediction: 0 True Class: 0\n",
            "Test 133 Prediction: 5 True Class: 5\n",
            "Test 134 Prediction: 5 True Class: 5\n",
            "Test 135 Prediction: 1 True Class: 1\n",
            "Test 136 Prediction: 9 True Class: 9\n",
            "Test 137 Prediction: 5 True Class: 5\n",
            "Test 138 Prediction: 7 True Class: 7\n",
            "Test 139 Prediction: 8 True Class: 8\n",
            "Test 140 Prediction: 1 True Class: 1\n",
            "Test 141 Prediction: 1 True Class: 1\n",
            "Test 142 Prediction: 0 True Class: 0\n",
            "Test 143 Prediction: 5 True Class: 5\n",
            "Test 144 Prediction: 2 True Class: 2\n",
            "Test 145 Prediction: 1 True Class: 1\n",
            "Test 146 Prediction: 1 True Class: 1\n",
            "Test 147 Prediction: 1 True Class: 1\n",
            "Test 148 Prediction: 1 True Class: 1\n",
            "Test 149 Prediction: 0 True Class: 0\n",
            "Test 150 Prediction: 5 True Class: 5\n",
            "Test 151 Prediction: 5 True Class: 5\n",
            "Test 152 Prediction: 6 True Class: 6\n",
            "Test 153 Prediction: 0 True Class: 0\n",
            "Test 154 Prediction: 2 True Class: 2\n",
            "Test 155 Prediction: 3 True Class: 3\n",
            "Test 156 Prediction: 3 True Class: 3\n",
            "Test 157 Prediction: 6 True Class: 6\n",
            "Test 158 Prediction: 3 True Class: 3\n",
            "Test 159 Prediction: 3 True Class: 3\n",
            "Test 160 Prediction: 7 True Class: 7\n",
            "Test 161 Prediction: 6 True Class: 6\n",
            "Test 162 Prediction: 5 True Class: 5\n",
            "Test 163 Prediction: 9 True Class: 9\n",
            "Test 164 Prediction: 8 True Class: 8\n",
            "Test 165 Prediction: 6 True Class: 6\n",
            "Test 166 Prediction: 2 True Class: 2\n",
            "Test 167 Prediction: 0 True Class: 0\n",
            "Test 168 Prediction: 4 True Class: 4\n",
            "Test 169 Prediction: 0 True Class: 0\n",
            "Test 170 Prediction: 9 True Class: 9\n",
            "Test 171 Prediction: 6 True Class: 6\n",
            "Test 172 Prediction: 4 True Class: 4\n",
            "Test 173 Prediction: 4 True Class: 4\n",
            "Test 174 Prediction: 1 True Class: 1\n",
            "Test 175 Prediction: 6 True Class: 5\n",
            "Test 176 Prediction: 2 True Class: 2\n",
            "Test 177 Prediction: 9 True Class: 4\n",
            "Test 178 Prediction: 8 True Class: 8\n",
            "Test 179 Prediction: 7 True Class: 7\n",
            "Test 180 Prediction: 4 True Class: 4\n",
            "Test 181 Prediction: 5 True Class: 5\n",
            "Test 182 Prediction: 9 True Class: 9\n",
            "Test 183 Prediction: 0 True Class: 0\n",
            "Test 184 Prediction: 8 True Class: 8\n",
            "Test 185 Prediction: 7 True Class: 7\n",
            "Test 186 Prediction: 4 True Class: 4\n",
            "Test 187 Prediction: 4 True Class: 4\n",
            "Test 188 Prediction: 6 True Class: 6\n",
            "Test 189 Prediction: 4 True Class: 4\n",
            "Test 190 Prediction: 5 True Class: 5\n",
            "Test 191 Prediction: 1 True Class: 1\n",
            "Test 192 Prediction: 6 True Class: 6\n",
            "Test 193 Prediction: 0 True Class: 0\n",
            "Test 194 Prediction: 2 True Class: 2\n",
            "Test 195 Prediction: 8 True Class: 8\n",
            "Test 196 Prediction: 2 True Class: 2\n",
            "Test 197 Prediction: 0 True Class: 0\n",
            "Test 198 Prediction: 2 True Class: 2\n",
            "Test 199 Prediction: 6 True Class: 4\n",
            "Done!\n",
            "Accuracy: 0.9200000000000007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjgxScRDJReL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "6f0b5a55-02da-408c-8866-6f00604e30f0"
      },
      "source": [
        "\"\"\" Random Forest.\n",
        "Implement Random Forest algorithm with TensorFlow, and apply it to classify \n",
        "handwritten digit images. This example is using the MNIST database of \n",
        "handwritten digits as training samples (http://yann.lecun.com/exdb/mnist/).\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
        "from tensorflow.python.ops import resources\n",
        "\n",
        "# Ignore all GPUs, tf random forest does not benefit from it.\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
        "\n",
        "# Parameters\n",
        "num_steps = 500 # Total steps to train\n",
        "batch_size = 1024 # The number of samples per batch\n",
        "num_classes = 10 # The 10 digits\n",
        "num_features = 784 # Each image is 28x28 pixels\n",
        "num_trees = 10\n",
        "max_nodes = 1000\n",
        "\n",
        "# Input and Target data\n",
        "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
        "# For random forest, labels must be integers (the class id)\n",
        "Y = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "# Random Forest Parameters\n",
        "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
        "                                      num_features=num_features,\n",
        "                                      num_trees=num_trees,\n",
        "                                      max_nodes=max_nodes).fill()\n",
        "\n",
        "# Build the Random Forest\n",
        "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
        "# Get training graph and loss\n",
        "train_op = forest_graph.training_graph(X, Y)\n",
        "loss_op = forest_graph.training_loss(X, Y)\n",
        "\n",
        "# Measure the accuracy\n",
        "infer_op, _, _ = forest_graph.inference_graph(X)\n",
        "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
        "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value) and forest resources\n",
        "init_vars = tf.group(tf.global_variables_initializer(),\n",
        "    resources.initialize_resources(resources.shared_resources()))\n",
        "\n",
        "# Start TensorFlow session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Run the initializer\n",
        "sess.run(init_vars)\n",
        "\n",
        "# Training\n",
        "for i in range(1, num_steps + 1):\n",
        "    # Prepare Data\n",
        "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
        "    if i % 50 == 0 or i == 1:\n",
        "        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n",
        "\n",
        "# Test Model\n",
        "test_x, test_y = mnist.test.images, mnist.test.labels\n",
        "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Constructing forest with params = \n",
            "INFO:tensorflow:{'num_trees': 10, 'max_nodes': 1000, 'bagging_fraction': 1.0, 'feature_bagging_fraction': 1.0, 'num_splits_to_consider': 28, 'max_fertile_nodes': 0, 'split_after_samples': 250, 'valid_leaf_threshold': 1, 'dominate_method': 'bootstrap', 'dominate_fraction': 0.99, 'model_name': 'all_dense', 'split_finish_name': 'basic', 'split_pruning_name': 'none', 'collate_examples': False, 'checkpoint_stats': False, 'use_running_stats_method': False, 'initialize_average_splits': False, 'inference_tree_paths': False, 'param_file': None, 'split_name': 'less_or_equal', 'early_finish_check_every_samples': 0, 'prune_every_samples': 0, 'num_classes': 10, 'num_features': 784, 'bagged_num_features': 784, 'bagged_features': None, 'regression': False, 'num_outputs': 1, 'num_output_columns': 11, 'base_random_seed': 0, 'leaf_model_type': 0, 'stats_model_type': 0, 'finish_type': 0, 'pruning_type': 0, 'split_type': 0}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/python/tensor_forest.py:529: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Step 1, Loss: -1.200000, Acc: 0.377930\n",
            "Step 50, Loss: -253.600006, Acc: 0.884766\n",
            "Step 100, Loss: -539.400024, Acc: 0.915039\n",
            "Step 150, Loss: -824.799988, Acc: 0.914062\n",
            "Step 200, Loss: -1001.000000, Acc: 0.932617\n",
            "Step 250, Loss: -1001.000000, Acc: 0.929688\n",
            "Step 300, Loss: -1001.000000, Acc: 0.935547\n",
            "Step 350, Loss: -1001.000000, Acc: 0.932617\n",
            "Step 400, Loss: -1001.000000, Acc: 0.918945\n",
            "Step 450, Loss: -1001.000000, Acc: 0.918945\n",
            "Step 500, Loss: -1001.000000, Acc: 0.927734\n",
            "Test Accuracy: 0.9214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMUvOHNzJSbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "753e2135-86f1-4677-f54a-a43f8878c918"
      },
      "source": [
        "\"\"\" Neural Network.\n",
        "A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\n",
        "implementation with TensorFlow. This example is using the MNIST database\n",
        "of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n",
        "Links:\n",
        "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.1\n",
        "num_steps = 500\n",
        "batch_size = 128\n",
        "display_step = 100\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, num_input])\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "# Create model\n",
        "def neural_net(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Construct model\n",
        "logits = neural_net(X)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
        "                                      Y: mnist.test.labels}))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-25-e0750f58f002>:64: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Step 1, Minibatch Loss= 11612.0527, Training Accuracy= 0.359\n",
            "Step 100, Minibatch Loss= 286.8232, Training Accuracy= 0.867\n",
            "Step 200, Minibatch Loss= 77.2709, Training Accuracy= 0.875\n",
            "Step 300, Minibatch Loss= 56.2657, Training Accuracy= 0.867\n",
            "Step 400, Minibatch Loss= 41.1424, Training Accuracy= 0.875\n",
            "Step 500, Minibatch Loss= 76.5497, Training Accuracy= 0.812\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.8441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAosvBG1JVQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e703edca-3d0e-4755-dcc7-c9c7fb7143de"
      },
      "source": [
        "\"\"\" Neural Network.\n",
        "A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\n",
        "implementation with TensorFlow. This example is using the MNIST database\n",
        "of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n",
        "This example is using TensorFlow layers, see 'neural_network_raw' example for\n",
        "a raw implementation with variables.\n",
        "Links:\n",
        "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.1\n",
        "num_steps = 1000\n",
        "batch_size = 128\n",
        "display_step = 100\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "\n",
        "# Define the neural network\n",
        "def neural_net(x_dict):\n",
        "    # TF Estimator input is a dict, in case of multiple inputs\n",
        "    x = x_dict['images']\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "# Define the model function (following TF Estimator Template)\n",
        "def model_fn(features, labels, mode):\n",
        "    # Build the neural network\n",
        "    logits = neural_net(features)\n",
        "\n",
        "    # Predictions\n",
        "    pred_classes = tf.argmax(logits, axis=1)\n",
        "    pred_probas = tf.nn.softmax(logits)\n",
        "\n",
        "    # If prediction mode, early return\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "    train_op = optimizer.minimize(loss_op,\n",
        "                                  global_step=tf.train.get_global_step())\n",
        "\n",
        "    # Evaluate the accuracy of the model\n",
        "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
        "\n",
        "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
        "    # the different ops for training, evaluating, ...\n",
        "    estim_specs = tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=pred_classes,\n",
        "        loss=loss_op,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops={'accuracy': acc_op})\n",
        "\n",
        "    return estim_specs\n",
        "\n",
        "# Build the Estimator\n",
        "model = tf.estimator.Estimator(model_fn)\n",
        "\n",
        "# Define the input function for training\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
        "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
        "# Train the Model\n",
        "model.train(input_fn, steps=num_steps)\n",
        "\n",
        "# Evaluate the Model\n",
        "# Define the input function for evaluating\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
        "    batch_size=batch_size, shuffle=False)\n",
        "# Use the Estimator 'evaluate' method\n",
        "e = model.evaluate(input_fn)\n",
        "\n",
        "print(\"Testing Accuracy:\", e['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6i75zjlo\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp6i75zjlo', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7aa7079eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-26-eb5df7f98c91>:39: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp6i75zjlo/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.5041943, step = 1\n",
            "INFO:tensorflow:global_step/sec: 149.395\n",
            "INFO:tensorflow:loss = 0.37786472, step = 101 (0.672 sec)\n",
            "INFO:tensorflow:global_step/sec: 162.726\n",
            "INFO:tensorflow:loss = 0.22042604, step = 201 (0.614 sec)\n",
            "INFO:tensorflow:global_step/sec: 159.983\n",
            "INFO:tensorflow:loss = 0.35569954, step = 301 (0.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 160.651\n",
            "INFO:tensorflow:loss = 0.3099713, step = 401 (0.621 sec)\n",
            "INFO:tensorflow:global_step/sec: 168.997\n",
            "INFO:tensorflow:loss = 0.18951344, step = 501 (0.592 sec)\n",
            "INFO:tensorflow:global_step/sec: 164.787\n",
            "INFO:tensorflow:loss = 0.27322042, step = 601 (0.607 sec)\n",
            "INFO:tensorflow:global_step/sec: 162.567\n",
            "INFO:tensorflow:loss = 0.24918851, step = 701 (0.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 160.29\n",
            "INFO:tensorflow:loss = 0.35916615, step = 801 (0.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 166.079\n",
            "INFO:tensorflow:loss = 0.21719135, step = 901 (0.601 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp6i75zjlo/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.31473678.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-02-09T22:50:41Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp6i75zjlo/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-02-09-22:50:42\n",
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9098, global_step = 1000, loss = 0.30685857\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmp6i75zjlo/model.ckpt-1000\n",
            "Testing Accuracy: 0.9098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likbEWHbJYS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "e883b02f-24a6-4fdd-d9e0-0257ff0056e8"
      },
      "source": [
        "\"\"\" Convolutional Neural Network.\n",
        "Build and train a convolutional neural network with TensorFlow.\n",
        "This example is using the MNIST database of handwritten digits\n",
        "(http://yann.lecun.com/exdb/mnist/)\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "num_steps = 200\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
        "\n",
        "\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
        "    # Reshape to match picture format [Height x Width x Channel]\n",
        "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
        "                                      Y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.0}))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-27-8721fd374e33>:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Step 1, Minibatch Loss= 63301.4102, Training Accuracy= 0.094\n",
            "Step 10, Minibatch Loss= 25273.3320, Training Accuracy= 0.211\n",
            "Step 20, Minibatch Loss= 10397.8799, Training Accuracy= 0.570\n",
            "Step 30, Minibatch Loss= 7092.5352, Training Accuracy= 0.695\n",
            "Step 40, Minibatch Loss= 6187.8481, Training Accuracy= 0.727\n",
            "Step 50, Minibatch Loss= 5221.3418, Training Accuracy= 0.695\n",
            "Step 60, Minibatch Loss= 2336.4172, Training Accuracy= 0.844\n",
            "Step 70, Minibatch Loss= 3360.6604, Training Accuracy= 0.812\n",
            "Step 80, Minibatch Loss= 2294.7212, Training Accuracy= 0.859\n",
            "Step 90, Minibatch Loss= 2616.2668, Training Accuracy= 0.859\n",
            "Step 100, Minibatch Loss= 2140.9917, Training Accuracy= 0.844\n",
            "Step 110, Minibatch Loss= 1450.8641, Training Accuracy= 0.906\n",
            "Step 120, Minibatch Loss= 1231.5055, Training Accuracy= 0.906\n",
            "Step 130, Minibatch Loss= 1762.9438, Training Accuracy= 0.930\n",
            "Step 140, Minibatch Loss= 2614.3462, Training Accuracy= 0.875\n",
            "Step 150, Minibatch Loss= 2064.6169, Training Accuracy= 0.883\n",
            "Step 160, Minibatch Loss= 1243.0667, Training Accuracy= 0.922\n",
            "Step 170, Minibatch Loss= 1824.0374, Training Accuracy= 0.898\n",
            "Step 180, Minibatch Loss= 2086.4736, Training Accuracy= 0.891\n",
            "Step 190, Minibatch Loss= 1254.8743, Training Accuracy= 0.906\n",
            "Step 200, Minibatch Loss= 2369.1455, Training Accuracy= 0.883\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.92578125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_6NqTzDJb06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "2b7f4e0c-5a24-4e76-e740-7cc1417194e2"
      },
      "source": [
        "\"\"\" Bi-directional Recurrent Neural Network.\n",
        "A Bi-directional Recurrent Neural Network (LSTM) implementation example using \n",
        "TensorFlow library. This example is using the MNIST database of handwritten \n",
        "digits (http://yann.lecun.com/exdb/mnist/)\n",
        "Links:\n",
        "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
        "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import numpy as np\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "'''\n",
        "To classify images using a bidirectional recurrent neural network, we consider\n",
        "every image row as a sequence of pixels. Because MNIST image shape is 28*28px,\n",
        "we will then handle 28 sequences of 28 steps for every sample.\n",
        "'''\n",
        "\n",
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "training_steps = 10000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 28 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 # timesteps\n",
        "num_hidden = 128 # hidden layer num of features\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
        "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def BiRNN(x, weights, biases):\n",
        "\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
        "\n",
        "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define lstm cells with tensorflow\n",
        "    # Forward direction cell\n",
        "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "    # Backward direction cell\n",
        "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "\n",
        "    # Get lstm cell output\n",
        "    try:\n",
        "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
        "                                              dtype=tf.float32)\n",
        "    except Exception: # Old TensorFlow version only returns outputs not states\n",
        "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
        "                                        dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "logits = BiRNN(X, weights, biases)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Reshape data to get 28 seq of 28 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 128 mnist test images\n",
        "    test_len = 128\n",
        "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "    test_label = mnist.test.labels[:test_len]\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-28-69f9f11b90d3>:65: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-28-69f9f11b90d3>:72: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:1610: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Step 1, Minibatch Loss= 2.3798, Training Accuracy= 0.211\n",
            "Step 200, Minibatch Loss= 2.1867, Training Accuracy= 0.195\n",
            "Step 400, Minibatch Loss= 1.9946, Training Accuracy= 0.297\n",
            "Step 600, Minibatch Loss= 1.6998, Training Accuracy= 0.445\n",
            "Step 800, Minibatch Loss= 1.6434, Training Accuracy= 0.477\n",
            "Step 1000, Minibatch Loss= 1.5643, Training Accuracy= 0.484\n",
            "Step 1200, Minibatch Loss= 1.5193, Training Accuracy= 0.539\n",
            "Step 1400, Minibatch Loss= 1.5523, Training Accuracy= 0.531\n",
            "Step 1600, Minibatch Loss= 1.4478, Training Accuracy= 0.539\n",
            "Step 1800, Minibatch Loss= 1.4015, Training Accuracy= 0.500\n",
            "Step 2000, Minibatch Loss= 1.4076, Training Accuracy= 0.484\n",
            "Step 2200, Minibatch Loss= 1.4426, Training Accuracy= 0.508\n",
            "Step 2400, Minibatch Loss= 1.1890, Training Accuracy= 0.555\n",
            "Step 2600, Minibatch Loss= 1.1187, Training Accuracy= 0.609\n",
            "Step 2800, Minibatch Loss= 1.1274, Training Accuracy= 0.648\n",
            "Step 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.625\n",
            "Step 3200, Minibatch Loss= 0.9978, Training Accuracy= 0.703\n",
            "Step 3400, Minibatch Loss= 1.0390, Training Accuracy= 0.703\n",
            "Step 3600, Minibatch Loss= 0.9628, Training Accuracy= 0.703\n",
            "Step 3800, Minibatch Loss= 0.9106, Training Accuracy= 0.781\n",
            "Step 4000, Minibatch Loss= 1.0037, Training Accuracy= 0.672\n",
            "Step 4200, Minibatch Loss= 1.0037, Training Accuracy= 0.711\n",
            "Step 4400, Minibatch Loss= 0.9075, Training Accuracy= 0.695\n",
            "Step 4600, Minibatch Loss= 0.8649, Training Accuracy= 0.789\n",
            "Step 4800, Minibatch Loss= 0.8021, Training Accuracy= 0.742\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}